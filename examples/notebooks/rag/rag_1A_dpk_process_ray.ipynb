{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841e533d-ebb3-406d-9da7-b19e2c5f5866",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #04D7FD; padding: 20px; text-align: left;\">\n",
    "    <h1 style=\"color: #000000; font-size: 36px; margin: 0;\">Demo: RAG with Data Prep Kit</h1>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15976e3",
   "metadata": {},
   "source": [
    "## Before Running the notebook\n",
    "\n",
    "Please complete [setting up python dev environment](./setup-python-dev-env.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ecf08-5f62-4b99-9347-8a0955843d21",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook will process PDF documents as part of RAG pipeline\n",
    "\n",
    "![](media/rag-overview-2.png)\n",
    "\n",
    "This notebook will perform steps 1, 2 and 3 in RAG pipeline.\n",
    "\n",
    "- [pdf2parquet](#pdf2parquet)\n",
    "- [Chunk documents](#chunking)\n",
    "- [Exact Dedup](#item3)\n",
    "- [Doc_ID generation](#item4)\n",
    "- [Fuzzy Dedup](#item5)\n",
    "- [Language detection](#item6)\n",
    "- [Doc quality](#item7)\n",
    "- [Filtering](#item8)\n",
    "- [Text encoder](#item9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b10be1",
   "metadata": {},
   "source": [
    "## Step-1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33345487",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration\n",
    "\n",
    "class MyConfig:\n",
    "    pass \n",
    "\n",
    "MY_CONFIG = MyConfig ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00e657c-36ab-4d77-8a02-4d374d9cedbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "## Input Data - configure this to the folder we want to process\n",
    "MY_CONFIG.INPUT_DATA_DIR = \"input_data/walmart-reports-1\"\n",
    "\n",
    "## RAY CONFIGURATION\n",
    "num_cpus_available =  os.cpu_count()\n",
    "# print (num_cpus_available)\n",
    "# MY_CONFIG.RAY_NUM_CPUS = num_cpus_available // 2  ## use half the available cores for processing\n",
    "MY_CONFIG.RAY_NUM_CPUS =  1\n",
    "# print (MY_CONFIG.RAY_NUM_CPUS)\n",
    "MY_CONFIG.RAY_MEMORY_GB = 2  # GB\n",
    "# MY_CONFIG.RAY_RUNTIME_WORKERS = num_cpus_available // 3\n",
    "MY_CONFIG.RAY_RUNTIME_WORKERS = 2\n",
    "\n",
    "## Embedding model\n",
    "MY_CONFIG.EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72510ae6-48b0-4b88-9e13-a623281c3a63",
   "metadata": {},
   "source": [
    "### Set input/output path variables for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ac8bee-0960-4309-b225-d7a211b14262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "\n",
    "## To process a different dataset, change it here\n",
    "pdf_input_folder = MY_CONFIG.INPUT_DATA_DIR \n",
    "\n",
    "if not os.path.exists(pdf_input_folder):\n",
    "    raise Exception (f\"❌ Input folder MY_CONFIG.INPUT_DATA_DIR = '{MY_CONFIG.INPUT_DATA_DIR}' not found\")\n",
    "\n",
    "output_folder = \"output\"\n",
    "\n",
    "parquet_data_output = os.path.join(output_folder, \"01_parquet_input\")\n",
    "chunk_out =  os.path.join(output_folder, \"02_chunk_out\")\n",
    "ededup_out =  os.path.join(output_folder, \"03_ededup_out\")\n",
    "doc_id_out =  os.path.join(output_folder, \"04_doc_id_out\")\n",
    "fdedup_out = os.path.join(output_folder, \"05_fdedup_out\")\n",
    "lang_out =  os.path.join(output_folder,\"06_lang_out\")\n",
    "dq_out = os.path.join(output_folder,\"07_dq_out\")\n",
    "filter_out = os.path.join(output_folder ,\"08_filter_out\")\n",
    "encoder_out = os.path.join(output_folder ,\"09_encoder_out\")\n",
    "\n",
    "output_folder_final = os.path.join(output_folder ,\"final_output\")\n",
    "\n",
    "## clear output folder\n",
    "shutil.rmtree(output_folder, ignore_errors=True)\n",
    "shutil.os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d976e-cb4c-4469-af39-4b7ea507e9d8",
   "metadata": {},
   "source": [
    "### Import Common python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66178913-42b8-426b-a2e9-9587268fd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Main repo root\n",
    "from utils import rootdir\n",
    "\n",
    "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
    "from data_processing.runtime.pure_python import PythonTransformLauncher\n",
    "from data_processing.utils import ParamsUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449e5c7-078c-4ad6-a2f6-21d39d4da3fb",
   "metadata": {},
   "source": [
    "<a id=\"pdf2parquet\"></a>\n",
    "\n",
    "## Step-2. Convert data to parquet using pdf2parquet\n",
    "\n",
    "This step is reading the input folder containing all PDF files and ingest them in a parquet table using the [Docling package](https://github.com/DS4SD/docling).\n",
    "The documents are converted into a JSON format which allows to easily chunk it in the later steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c574c4-9dc4-4dab-9ad6-b5338207e67a",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "482605b2-d814-456d-9195-49a2ec454ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing input='input_data/walmart-reports-1' --> output='output/01_parquet_input'\n"
     ]
    }
   ],
   "source": [
    "input_folder = pdf_input_folder\n",
    "output_folder =  parquet_data_output\n",
    "\n",
    "print (f\"Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb15f02-ab5c-4525-a536-cfa1fd2ba70b",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0cd8ebd-bf71-42d6-a397-8df0c7b66a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:26:06 INFO - Running locally\n",
      "23:26:06 INFO - pdf2parquet parameters are : {'artifacts_path': None, 'contents_type': <pdf2parquet_contents_types.JSON: 'application/json'>, 'do_table_structure': True, 'do_ocr': False}\n",
      "23:26:06 INFO - data factory data_ is using local data access: input_folder - input_data/walmart-reports-1 output_folder - output/01_parquet_input\n",
      "23:26:06 INFO - data factory data_ max_files -1, n_sample -1\n",
      "23:26:06 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.pdf'], files to checkpoint ['.parquet']\n",
      "23:26:06 INFO - pipeline id pipeline_id\n",
      "23:26:06 INFO - code location {'github': 'github', 'commit_hash': '12345', 'path': 'path'}\n",
      "23:26:06 INFO - number of workers 2 worker options {'num_cpus': 1, 'memory': 2147483648, 'max_restarts': -1}\n",
      "23:26:06 INFO - actor creation delay 0\n",
      "23:26:06 INFO - job details {'job category': 'preprocessing', 'job name': 'pdf2parquet', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-26 23:26:08,091\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=179877)\u001b[0m 23:26:11 INFO - orchestrator started at 2024-08-26 23:26:11\n",
      "\u001b[36m(orchestrate pid=179877)\u001b[0m 23:26:11 INFO - Number of files is 3, source profile {'max_file_size': 4.640201568603516, 'min_file_size': 1.5370569229125977, 'total_file_size': 10.817460060119629}\n",
      "\u001b[36m(orchestrate pid=179877)\u001b[0m 23:26:11 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 13.796502686105669, 'object_store': 6.898251342587173}\n",
      "\u001b[36m(orchestrate pid=179877)\u001b[0m 23:26:11 INFO - Number of workers - 2 with {'num_cpus': 1, 'memory': 2147483648, 'max_restarts': -1} each\n",
      "\u001b[36m(RayTransformFileProcessor pid=180735)\u001b[0m 23:26:15 INFO - Initializing models\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 23045.63it/s]\n",
      "\u001b[36m(RayTransformFileProcessor pid=180735)\u001b[0m /home/sujee/apps/anaconda3/envs/data-prep-kit-2/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "\u001b[36m(RayTransformFileProcessor pid=180735)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(RayTransformFileProcessor pid=180735)\u001b[0m /home/sujee/apps/anaconda3/envs/data-prep-kit-2/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "\u001b[36m(RayTransformFileProcessor pid=180735)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[36m(RayTransformFileProcessor pid=180735)\u001b[0m /home/sujee/apps/anaconda3/envs/data-prep-kit-2/lib/python3.11/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "\u001b[36m(RayTransformFileProcessor pid=180735)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "\u001b[36m(RayTransformFileProcessor pid=180735)\u001b[0m /home/sujee/apps/anaconda3/envs/data-prep-kit-2/lib/python3.11/site-packages/docling_ibm_models/tableformer/models/common/base_model.py:260: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(RayTransformFileProcessor pid=180735)\u001b[0m   saved_model = torch.load(checkpoint_file, map_location=self._device)\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m 2024-08-26 23:34:23.307 ( 490.574s) [        3C687740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-unselected\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m 23:26:15 INFO - Initializing models\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 137196.86it/s]\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m /home/sujee/apps/anaconda3/envs/data-prep-kit-2/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m /home/sujee/apps/anaconda3/envs/data-prep-kit-2/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m /home/sujee/apps/anaconda3/envs/data-prep-kit-2/lib/python3.11/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m /home/sujee/apps/anaconda3/envs/data-prep-kit-2/lib/python3.11/site-packages/docling_ibm_models/tableformer/models/common/base_model.py:260: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m   saved_model = torch.load(checkpoint_file, map_location=self._device)\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m 2024-08-26 23:34:27.846 ( 495.113s) [        3C687740]          crf_model.cpp:2096   ERR| sequence is too long: 1000 > 1011\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m 2024-08-26 23:34:28.627 ( 495.894s) [        3C687740]          crf_model.cpp:2096   ERR| sequence is too long: 1000 > 1037\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m 2024-08-26 23:34:28.751 ( 496.018s) [        3C687740]          crf_model.cpp:2096   ERR| sequence is too long: 1000 > 1176\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m 2024-08-26 23:34:29.273 ( 496.540s) [        3C687740]          crf_model.cpp:2096   ERR| sequence is too long: 1000 > 1321\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m 2024-08-26 23:34:29.899 ( 497.166s) [        3C687740]          crf_model.cpp:2096   ERR| sequence is too long: 1000 > 1103\n",
      "\u001b[36m(RayTransformFileProcessor pid=180735)\u001b[0m 2024-08-26 23:34:43.339 ( 510.651s) [        F6636740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-unselected\n",
      "\u001b[36m(RayTransformFileProcessor pid=180735)\u001b[0m 2024-08-26 23:34:43.339 ( 510.651s) [        F6636740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-selected\n",
      "\u001b[36m(RayTransformFileProcessor pid=180735)\u001b[0m 2024-08-26 23:34:43.339 ( 510.651s) [        F6636740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-unselected\n",
      "\u001b[36m(RayTransformFileProcessor pid=180735)\u001b[0m 2024-08-26 23:34:43.339 ( 510.651s) [        F6636740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-unselected\n",
      "\u001b[36m(RayTransformFileProcessor pid=180735)\u001b[0m 2024-08-26 23:34:43.339 ( 510.651s) [        F6636740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-selected\n",
      "\u001b[36m(orchestrate pid=179877)\u001b[0m 23:34:44 INFO - Completed 1 files in 8.540570064385731 min\n",
      "\u001b[36m(orchestrate pid=179877)\u001b[0m 23:34:44 INFO - Completed 1 files (33.333333333333336%)  in 8.540573100248972 min. Waiting for completion\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m 2024-08-26 23:40:41.723 ( 868.990s) [        3C687740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-unselected\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m 2024-08-26 23:40:43.540 ( 870.807s) [        3C687740]          crf_model.cpp:2096   ERR| sequence is too long: 1000 > 1011\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m 2024-08-26 23:40:44.106 ( 871.372s) [        3C687740]          crf_model.cpp:2096   ERR| sequence is too long: 1000 > 1037\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m 2024-08-26 23:40:44.182 ( 871.448s) [        3C687740]          crf_model.cpp:2096   ERR| sequence is too long: 1000 > 1176\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m 2024-08-26 23:40:44.497 ( 871.763s) [        3C687740]          crf_model.cpp:2096   ERR| sequence is too long: 1000 > 1321\n",
      "\u001b[36m(RayTransformFileProcessor pid=180734)\u001b[0m 2024-08-26 23:40:44.859 ( 872.126s) [        3C687740]          crf_model.cpp:2096   ERR| sequence is too long: 1000 > 1103\n",
      "\u001b[36m(orchestrate pid=179877)\u001b[0m 23:40:51 INFO - Completed processing 3 files in 14.6603253642718 min\n",
      "\u001b[36m(orchestrate pid=179877)\u001b[0m 23:40:51 INFO - done flushing in 0.0010380744934082031 sec\n",
      "23:41:01 INFO - Completed execution in 14.920970439910889 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.09 s, sys: 1.85 s, total: 6.94 s\n",
      "Wall time: 14min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import ast\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pdf2parquet_transform import (\n",
    "    pdf2parquet_contents_type_cli_param,\n",
    "    pdf2parquet_contents_types,\n",
    ")\n",
    "from pdf2parquet_transform_python import Pdf2ParquetPythonTransformConfiguration\n",
    "from pdf2parquet_transform_ray import Pdf2ParquetRayTransformConfiguration\n",
    "\n",
    "from data_processing.utils import GB, ParamsUtils\n",
    "\n",
    "\n",
    "# create parameters\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS, \"memory\": MY_CONFIG.RAY_MEMORY_GB * GB}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "ingest_config = {\n",
    "    pdf2parquet_contents_type_cli_param: pdf2parquet_contents_types.JSON,\n",
    "}\n",
    "\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    \"data_files_to_use\": ast.literal_eval(\"['.pdf']\"),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    \"runtime_pipeline_id\": \"pipeline_id\",\n",
    "    \"runtime_job_id\": \"job_id\",\n",
    "    \"runtime_code_location\": ParamsUtils.convert_to_ast(code_location),\n",
    "}\n",
    "\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=(params | ingest_config))\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(Pdf2ParquetRayTransformConfiguration())\n",
    "# launcher = PythonTransformLauncher(Pdf2ParquetPythonTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code != 0:\n",
    "    raise Exception (\"❌ Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd5d13",
   "metadata": {},
   "source": [
    "<a id=\"chunking\"></a>\n",
    "\n",
    "##  Step-3: Doc chunks\n",
    "\n",
    "Split the documents in chunks, according to their layout segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d86f84e",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81843d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing input='output/01_parquet_input' --> output='output/02_chunk_out'\n"
     ]
    }
   ],
   "source": [
    "## For this stage the input is the folder containing parquet data which is output from the ingest2parquet tool\n",
    "\n",
    "input_folder = parquet_data_output\n",
    "output_folder = chunk_out\n",
    "\n",
    "print (f\"Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35af440",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d34705d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/sujee/apps/anaconda3/envs/data-prep-\n",
      "[nltk_data]     kit-2/lib/python3.11/site-\n",
      "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "23:41:03 INFO - Running locally\n",
      "23:41:03 INFO - doc_chunk parameters are : {'chunking_type': <chunking_types.DL_JSON: 'dl_json'>, 'content_column_name': 'contents', 'output_chunk_column_name': 'contents', 'output_jsonpath_column_name': 'doc_jsonpath', 'output_pageno_column_name': 'page_number', 'output_bbox_column_name': 'bbox'}\n",
      "23:41:03 INFO - data factory data_ is using local data access: input_folder - output/01_parquet_input output_folder - output/02_chunk_out\n",
      "23:41:03 INFO - data factory data_ max_files -1, n_sample -1\n",
      "23:41:03 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "23:41:03 INFO - pipeline id pipeline_id\n",
      "23:41:03 INFO - code location None\n",
      "23:41:03 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "23:41:03 INFO - actor creation delay 0\n",
      "23:41:03 INFO - job details {'job category': 'preprocessing', 'job name': 'doc_chunk', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-26 23:41:05,528\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=191205)\u001b[0m [nltk_data] Downloading package punkt_tab to\n",
      "\u001b[36m(orchestrate pid=191205)\u001b[0m [nltk_data]     /home/sujee/apps/anaconda3/envs/data-prep-\n",
      "\u001b[36m(orchestrate pid=191205)\u001b[0m [nltk_data]     kit-2/lib/python3.11/site-\n",
      "\u001b[36m(orchestrate pid=191205)\u001b[0m [nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
      "\u001b[36m(orchestrate pid=191205)\u001b[0m [nltk_data]   Package punkt_tab is already up-to-date!\n",
      "\u001b[36m(orchestrate pid=191205)\u001b[0m 23:41:07 INFO - orchestrator started at 2024-08-26 23:41:07\n",
      "\u001b[36m(orchestrate pid=191205)\u001b[0m 23:41:07 INFO - Number of files is 3, source profile {'max_file_size': 0.3565502166748047, 'min_file_size': 0.35198307037353516, 'total_file_size': 1.060612678527832}\n",
      "\u001b[36m(orchestrate pid=191205)\u001b[0m 23:41:07 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 14.05144729744643, 'object_store': 7.025723647326231}\n",
      "\u001b[36m(orchestrate pid=191205)\u001b[0m 23:41:07 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=191205)\u001b[0m 23:41:09 INFO - Completed 1 files in 0.029376383622487387 min\n",
      "\u001b[36m(orchestrate pid=191205)\u001b[0m 23:41:09 INFO - Completed 1 files (33.333333333333336%)  in 0.029378175735473633 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=191205)\u001b[0m 23:41:09 INFO - Completed processing 3 files in 0.032480943202972415 min\n",
      "\u001b[36m(orchestrate pid=191205)\u001b[0m 23:41:09 INFO - done flushing in 0.0014421939849853516 sec\n",
      "23:41:19 INFO - Completed execution in 0.26104713678359986 min, execution result 0\n",
      "\u001b[36m(RayTransformFileProcessor pid=192059)\u001b[0m [nltk_data] Downloading package punkt_tab to\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayTransformFileProcessor pid=192059)\u001b[0m [nltk_data]     kit-2/lib/python3.11/site-\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTransformFileProcessor pid=192059)\u001b[0m [nltk_data]     packages/llama_index/core/_static/nltk_cache...\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTransformFileProcessor pid=192059)\u001b[0m [nltk_data]   Package punkt_tab is already up-to-date!\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.19 s, sys: 1.3 s, total: 2.49 s\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Import doc_json_chunk transform configuration\n",
    "from doc_chunk_transform_ray import DocChunkRayTransformConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # doc_chunk arguments\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(DocChunkRayTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code != 0:\n",
    "    raise Exception (\"❌ Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4692975c-49ff-41ae-810e-0f5bc0bbdc53",
   "metadata": {},
   "source": [
    "## Step-4: Exact Dedup\n",
    "\n",
    "Remove documents having identical code to remove bias in the training data. On the content of each document, a SHA256 hash is computed,\n",
    "followed by de-duplication of record having identical hashes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfd3a2-a236-4143-bcfc-15804f1da7fe",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58a0e1f6-ff53-40aa-96b1-096ade4bd1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing input='output/02_chunk_out' --> output='output/03_ededup_out'\n"
     ]
    }
   ],
   "source": [
    "## For this stage the input is the folder containing parquet data which is output from the ingest2parquet tool\n",
    "\n",
    "input_folder = chunk_out\n",
    "output_folder = ededup_out\n",
    "\n",
    "print (f\"Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661cb37-39c7-4b09-a784-925bfa9eaf1e",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a624b2b2-faad-4325-ac7d-53a840f564ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:41:20 INFO - Running locally\n",
      "23:41:20 INFO - exact dedup params are {'doc_column': 'contents', 'hash_cpu': 0.5, 'num_hashes': 2}\n",
      "23:41:20 INFO - data factory data_ is using local data access: input_folder - output/02_chunk_out output_folder - output/03_ededup_out\n",
      "23:41:20 INFO - data factory data_ max_files -1, n_sample -1\n",
      "23:41:20 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "23:41:20 INFO - pipeline id pipeline_id\n",
      "23:41:20 INFO - code location None\n",
      "23:41:20 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "23:41:20 INFO - actor creation delay 0\n",
      "23:41:20 INFO - job details {'job category': 'preprocessing', 'job name': 'ededup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-26 23:41:22,676\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=192752)\u001b[0m 23:41:23 INFO - orchestrator started at 2024-08-26 23:41:23\n",
      "\u001b[36m(orchestrate pid=192752)\u001b[0m 23:41:23 INFO - Number of files is 3, source profile {'max_file_size': 0.20615005493164062, 'min_file_size': 0.19641399383544922, 'total_file_size': 0.5990447998046875}\n",
      "\u001b[36m(orchestrate pid=192752)\u001b[0m 23:41:23 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 13.974327851086855, 'object_store': 6.987163924612105}\n",
      "\u001b[36m(orchestrate pid=192752)\u001b[0m 23:41:23 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=192752)\u001b[0m 23:41:24 INFO - Completed 1 files in 0.010657294591267904 min\n",
      "\u001b[36m(orchestrate pid=192752)\u001b[0m 23:41:24 INFO - Completed 1 files (33.333333333333336%)  in 0.010659563541412353 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=192752)\u001b[0m 23:41:24 INFO - Completed processing 3 files in 0.011292322476704916 min\n",
      "\u001b[36m(orchestrate pid=192752)\u001b[0m 23:41:24 INFO - done flushing in 0.0009121894836425781 sec\n",
      "23:41:34 INFO - Completed execution in 0.22213805913925172 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 104 ms, sys: 121 ms, total: 225 ms\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Import ededup transform configuration\n",
    "from ededup_transform_ray import EdedupRayTransformConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # ededup parameters\n",
    "    \"ededup_hash_cpu\": 0.5,\n",
    "    \"ededup_num_hashes\": 2,\n",
    "    \"ededup_doc_column\": \"contents\",\n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(EdedupRayTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code != 0:\n",
    "    raise Exception (\"❌ Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f4d00-33bb-4d9a-9f34-4d7f3ee0b7bc",
   "metadata": {},
   "source": [
    "## Step-5:  DOC ID generation\n",
    "\n",
    "This transform annotates documents with document \"ids\". It supports the following transformations of the original data:\n",
    "\n",
    " - Adding document hash: this enables the addition of a document hash-based id to the data. The hash is calculated with `hashlib.sha256(doc.encode(\"utf-8\")).hexdigest()`. To enable this annotation, set hash_column to the name of the column, where you want to store it.\n",
    " - Adding integer document id: this allows the addition of an integer document id to the data that is unique across all rows in all tables provided to the transform() method. To enable this annotation, set int_id_column to the name of the column, where you want to store it. **This is a pre-requisite for fuzzy dedup** in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6f62394-fbde-495c-bbbb-83161b006bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing input='output/03_ededup_out' --> output='output/04_doc_id_out'\n"
     ]
    }
   ],
   "source": [
    "# Input for this stage is the output of exact dedeup component\n",
    "# output of this component makes it possible for fdedup component to run on data.\n",
    "\n",
    "input_folder = ededup_out\n",
    "output_folder = doc_id_out\n",
    "print (f\"Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6daf36d-686c-4e0a-aabf-ce55f999bb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:41:35 INFO - Running locally\n",
      "23:41:35 INFO - Doc id parameters are : {'doc_column': 'contents', 'hash_column': 'hash_column', 'int_column': 'int_id_column'}\n",
      "23:41:35 INFO - data factory data_ is using local data access: input_folder - output/03_ededup_out output_folder - output/04_doc_id_out\n",
      "23:41:35 INFO - data factory data_ max_files -1, n_sample -1\n",
      "23:41:35 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "23:41:35 INFO - pipeline id pipeline_id\n",
      "23:41:35 INFO - code location None\n",
      "23:41:35 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "23:41:35 INFO - actor creation delay 0\n",
      "23:41:35 INFO - job details {'job category': 'preprocessing', 'job name': 'doc_id', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-26 23:41:37,447\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=194299)\u001b[0m 23:41:38 INFO - orchestrator started at 2024-08-26 23:41:38\n",
      "\u001b[36m(orchestrate pid=194299)\u001b[0m 23:41:38 INFO - Number of files is 3, source profile {'max_file_size': 0.2052764892578125, 'min_file_size': 0.003185272216796875, 'total_file_size': 0.4036588668823242}\n",
      "\u001b[36m(orchestrate pid=194299)\u001b[0m 23:41:38 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 13.956717683002353, 'object_store': 6.978358840569854}\n",
      "\u001b[36m(orchestrate pid=194299)\u001b[0m 23:41:38 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=194299)\u001b[0m 23:41:38 INFO - Completed 1 files in 0.010745545228322348 min\n",
      "\u001b[36m(orchestrate pid=194299)\u001b[0m 23:41:38 INFO - Completed 1 files (33.333333333333336%)  in 0.010747953255971273 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=194299)\u001b[0m 23:41:38 INFO - Completed processing 3 files in 0.0108474334081014 min\n",
      "\u001b[36m(orchestrate pid=194299)\u001b[0m 23:41:38 INFO - done flushing in 0.0009553432464599609 sec\n",
      "\u001b[36m(RayTransformFileProcessor pid=195132)\u001b[0m 23:41:38 WARNING - table is empty, skipping processing\n",
      "23:41:48 INFO - Completed execution in 0.22191858291625977 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 99.3 ms, sys: 139 ms, total: 239 ms\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from doc_id_transform_ray import DocIDRayTransformConfiguration\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # doc id configuration\n",
    "    \"doc_id_doc_column\": \"contents\",\n",
    "    \"doc_id_hash_column\": \"hash_column\",\n",
    "    \"doc_id_int_column\": \"int_id_column\",\n",
    "}\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = RayTransformLauncher(DocIDRayTransformConfiguration())\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code != 0:\n",
    "    raise Exception (\"❌ Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85309751-8556-41c6-ac32-84acc941bc8d",
   "metadata": {},
   "source": [
    "## Step-6: Fuzzy Dedup\n",
    "\n",
    "Post exact deduplication, fuzzy deduplication is applied with\n",
    "the goal of removing code files that may have slight variations and thereby unbiasing\n",
    "the data further. Small variations are quite commonly seen in code data in the form\n",
    "of variations in the values of variables, addittion of logging statements etc. Find near-\n",
    "duplicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf574a3-b287-419c-9c86-07b828b41ca6",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e431c8c-c7c7-48de-ba5f-2c4649c35399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing input='output/04_doc_id_out' --> output='output/05_fdedup_out'\n"
     ]
    }
   ],
   "source": [
    "## Input to this component is the output of doc_id generator component. \n",
    "\n",
    "input_folder = doc_id_out\n",
    "output_folder = fdedup_out\n",
    "print (f\"Processing input='{input_folder}' --> output='{output_folder}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c82a8f-b513-4fe5-b172-d41b104b54f3",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3864ff77-e9a8-48f7-973b-c3b3aef1a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:41:50 INFO - Running locally\n",
      "23:41:50 INFO - fuzzy dedup params are {'doc_column': 'contents', 'id_column': 'int_id_column', 'cluster_column': 'hash_column', 'bucket_cpu': 0.5, 'mhash_cpu': 0.5, 'doc_cpu': 0.5, 'num_doc_actors': 2, 'num_minhash_actors': 1, 'num_bucket_actors': 1, 'num_preprocessors': 2, 'num_permutations': 64, 'threshold': 0.8, 'shingles_size': 5, 'delimiters': ' ', 'snapshot_delay': 1, 'use_bucket_snapshot': False, 'use_doc_snapshot': False, 'random_delay_limit': 10, 'worker_options': {'num_cpus': 1}}\n",
      "23:41:50 INFO - data factory data_ is using local data access: input_folder - output/04_doc_id_out output_folder - output/05_fdedup_out\n",
      "23:41:50 INFO - data factory data_ max_files -1, n_sample -1\n",
      "23:41:50 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "23:41:50 INFO - pipeline id pipeline_id\n",
      "23:41:50 INFO - code location None\n",
      "23:41:50 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "23:41:50 INFO - actor creation delay 0\n",
      "23:41:50 INFO - job details {'job category': 'preprocessing', 'job name': 'fdedup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-26 23:41:52,275\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:41:53 INFO - orchestrator started at 2024-08-26 23:41:53\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:41:53 INFO - Number of files is 2, source profile {'max_file_size': 0.25394725799560547, 'min_file_size': 0.24273204803466797, 'total_file_size': 0.49667930603027344}\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:41:53 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 13.973316192626953, 'object_store': 6.986658096313477}\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:41:53 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:41:53 INFO - starting run from the beginning\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:41:53 INFO - continuing from the very beginning\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:41:53 INFO - Fuzzy: num buckets 5, bucket length 11\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:41:53 INFO - created 1 bucket actors\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:41:53 INFO - created 1 minhash actors\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:41:53 INFO - Table preprocessing uses 2 readers\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:41:53 INFO - created 2 table processor actors\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:41:53 INFO - Completed 0 files (0.0%)  in 4.732608795166016e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:42:02 INFO - Completed processing 2 files in 0.14848819176355998 min\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:42:02 INFO - creating minhash snapshots\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:42:03 INFO - minhash snapshots created\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:42:03 INFO - creating bucket snapshots\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:42:04 INFO - bucket snapshots created\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:42:04 INFO - created 2 document actors\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:42:04 INFO - created 2 bucket processor actors\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:42:04 INFO - created bucket processor invoker\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:42:04 INFO - added invoker to bucket collectors\n",
      "\u001b[36m(BucketsHash pid=196659)\u001b[0m 23:42:04 INFO - processing buckets 0 long, 6569 short\n",
      "\u001b[36m(BucketsHash pid=196659)\u001b[0m 23:42:04 INFO - Done submitting long buckets\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:42:04 INFO - Done processing buckets in 0.011369828383127849 min\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:42:04 INFO - creating document snapshots\n",
      "\u001b[36m(BucketsHashProcessorInvoker pid=197133)\u001b[0m 23:42:04 INFO - Waiting bucket processing completion. Submitted requests 66\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:42:06 INFO - document snapshots created\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:42:06 INFO - Completed 0 files (0.0%)  in 5.316734313964844e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:42:14 INFO - Completed processing 2 files in 0.12845727602640789 min\n",
      "\u001b[36m(orchestrate pid=195825)\u001b[0m 23:42:14 INFO - done flushing in 0.0018665790557861328 sec\n",
      "23:42:24 INFO - Completed execution in 0.5690935571988424 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 199 ms, sys: 183 ms, total: 383 ms\n",
      "Wall time: 35.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.utils import ParamsUtils\n",
    "from fdedup_transform_ray import FdedupRayTransformConfiguration\n",
    "\n",
    "# create parameters\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # Orchestration parameters\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # columns used\n",
    "    \"fdedup_doc_column\": \"contents\",\n",
    "    \"fdedup_id_column\": \"int_id_column\",\n",
    "    \"fdedup_cluster_column\": \"hash_column\",\n",
    "    # infrastructure\n",
    "    \"fdedup_bucket_cpu\": 0.5,\n",
    "    \"fdedup_doc_cpu\": 0.5,\n",
    "    \"fdedup_mhash_cpu\": 0.5,\n",
    "    \"fdedup_num_doc_actors\": 2,\n",
    "    \"fdedup_num_bucket_actors\": 1,\n",
    "    \"fdedup_num_minhash_actors\": 1,\n",
    "    \"fdedup_num_preprocessors\": 2,\n",
    "    # fuzzy parameters\n",
    "    \"fdedup_num_permutations\": 64,\n",
    "    \"fdedup_threshold\": 0.8,\n",
    "    \"fdedup_shingles_size\": 5,\n",
    "    \"fdedup_delimiters\": \" \"\n",
    "}\n",
    "\n",
    "# Pass commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = RayTransformLauncher(FdedupRayTransformConfiguration())\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code != 0:\n",
    "    raise Exception (\"❌ Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d1d761-62a7-4a12-ad23-b2c268ad8ed2",
   "metadata": {},
   "source": [
    "## Step-7: Language identification\n",
    "\n",
    "This transform identifies the language of the document components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db05e1e-4c62-4367-93ca-b2ddff95e4b4",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8ec4fb6-fa62-45d1-9aa1-596d7182b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_folder = fdedup_out\n",
    "# output_folder = lang_out \n",
    "# print (f\"Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07e7e5a-064f-4dca-a017-4211f7a3e980",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48dbb2a3-a6f4-4a3d-bb2f-8491fd063611",
   "metadata": {},
   "outputs": [],
   "source": [
    "## disabling this for now\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# from data_processing.utils import ParamsUtils\n",
    "# from lang_id_transform import (\n",
    "#     content_column_name_cli_param,\n",
    "#     model_credential_cli_param,\n",
    "#     model_kind_cli_param,\n",
    "#     model_url_cli_param,\n",
    "# )\n",
    "# from lang_models import KIND_FASTTEXT\n",
    "# from lang_id_transform_ray import LangIdentificationRayTransformConfiguration\n",
    "\n",
    "\n",
    "# local_conf = {\n",
    "#     \"input_folder\": input_folder,\n",
    "#     \"output_folder\": output_folder,\n",
    "# }\n",
    "# worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "# langid_config = {\n",
    "#     model_credential_cli_param: None, #\"PUT YOUR OWN HUGGINGFACE CREDENTIAL\",\n",
    "#     model_kind_cli_param: KIND_FASTTEXT,\n",
    "#     model_url_cli_param: \"facebook/fasttext-language-identification\",\n",
    "#     # content_column_name_cli_param: \"text\",\n",
    "# }\n",
    "# params = {\n",
    "#     # where to run\n",
    "#     \"run_locally\": True,\n",
    "#     # Data access. Only required parameters are specified\n",
    "#     \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "#     # orchestrator\n",
    "#     \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "#     \"runtime_num_workers\": 1,\n",
    "#     # language selection specific parameters\n",
    "#     **langid_config,\n",
    "# }\n",
    "\n",
    "# sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# # create launcher\n",
    "# launcher = RayTransformLauncher(LangIdentificationRayTransformConfiguration())\n",
    "# return_code = launcher.launch()\n",
    "\n",
    "# if return_code != 0:\n",
    "#     raise Exception (\"❌ Ray job failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646cbb7-3046-44c0-827d-d102d3ff7cb8",
   "metadata": {},
   "source": [
    "## Step-8: Document Quality\n",
    "\n",
    "TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e985668-848b-4633-b0d8-9fe70ada0c91",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f080011-c9fe-430e-9ecc-f2220d2c8d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing input='output/05_fdedup_out' --> output='output/07_dq_out'\n"
     ]
    }
   ],
   "source": [
    "# input_folder = lang_out\n",
    "input_folder = fdedup_out\n",
    "output_folder = dq_out\n",
    "print (f\"Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02982c5-f398-4a1a-a9fe-42d7ae748c7c",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29319fb9-b0d8-4f86-9bc5-b92960ad8ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:42:25 INFO - Running locally\n",
      "23:42:25 INFO - doc_quality parameters are : {'text_lang': 'en', 'doc_content_column': 'contents', 'bad_word_filepath': '/home/sujee/my-stuff/projects/ai-alliance/data-prep-kit-sujee/transforms/language/doc_quality/python/ldnoobw/en', 's3_cred': None, 'docq_data_factory': <data_processing.data_access.data_access_factory.DataAccessFactory object at 0x768aa857ca90>}\n",
      "23:42:25 INFO - data factory docq_ is using local configuration without input/output path\n",
      "23:42:25 INFO - data factory docq_ max_files -1, n_sample -1\n",
      "23:42:25 INFO - data factory docq_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "23:42:25 INFO - data factory data_ is using local data access: input_folder - output/05_fdedup_out output_folder - output/07_dq_out\n",
      "23:42:25 INFO - data factory data_ max_files -1, n_sample -1\n",
      "23:42:25 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "23:42:25 INFO - pipeline id pipeline_id\n",
      "23:42:25 INFO - code location None\n",
      "23:42:25 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "23:42:25 INFO - actor creation delay 0\n",
      "23:42:25 INFO - job details {'job category': 'preprocessing', 'job name': 'docq', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-26 23:42:27,774\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=198006)\u001b[0m 23:42:28 INFO - orchestrator started at 2024-08-26 23:42:28\n",
      "\u001b[36m(orchestrate pid=198006)\u001b[0m 23:42:28 INFO - Number of files is 2, source profile {'max_file_size': 0.20832252502441406, 'min_file_size': 0.19798946380615234, 'total_file_size': 0.4063119888305664}\n",
      "\u001b[36m(orchestrate pid=198006)\u001b[0m 23:42:28 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 13.965783691965044, 'object_store': 6.9828918455168605}\n",
      "\u001b[36m(orchestrate pid=198006)\u001b[0m 23:42:28 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=198006)\u001b[0m 23:42:28 INFO - Completed 0 files (0.0%)  in 8.141994476318359e-06 min. Waiting for completion\n",
      "\u001b[36m(RayTransformFileProcessor pid=198846)\u001b[0m 23:42:29 INFO - Load badwords found locally from /home/sujee/my-stuff/projects/ai-alliance/data-prep-kit-sujee/transforms/language/doc_quality/python/ldnoobw/en\n",
      "\u001b[36m(orchestrate pid=198006)\u001b[0m 23:42:29 INFO - Completed processing 2 files in 0.020508718490600587 min\n",
      "\u001b[36m(orchestrate pid=198006)\u001b[0m 23:42:29 INFO - done flushing in 0.0012691020965576172 sec\n",
      "23:42:39 INFO - Completed execution in 0.23126235802968342 min, execution result 0\n",
      "\u001b[36m(RayTransformFileProcessor pid=198847)\u001b[0m 23:42:29 INFO - Load badwords found locally from /home/sujee/my-stuff/projects/ai-alliance/data-prep-kit-sujee/transforms/language/doc_quality/python/ldnoobw/en\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 131 ms, sys: 137 ms, total: 268 ms\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from doc_quality_transform import (\n",
    "    text_lang_cli_param,\n",
    "    doc_content_column_cli_param,\n",
    "    bad_word_filepath_cli_param,\n",
    ")\n",
    "from doc_quality_transform_ray import DocQualityRayTransformConfiguration\n",
    "from data_processing.utils import ParamsUtils\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "doc_quality_basedir = os.path.join(rootdir, \"transforms\", \"language\", \"doc_quality\", \"python\")\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    \"runtime_pipeline_id\": \"pipeline_id\",\n",
    "    \"runtime_job_id\": \"job_id\",\n",
    "    \"runtime_creation_delay\": 0,\n",
    "    # doc quality configuration\n",
    "    text_lang_cli_param: \"en\",\n",
    "    doc_content_column_cli_param: \"contents\",\n",
    "    bad_word_filepath_cli_param: os.path.join(doc_quality_basedir, \"ldnoobw\", \"en\"),\n",
    "}\n",
    "\n",
    "\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(DocQualityRayTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code != 0:\n",
    "    raise Exception (\"❌ Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec9839-13b4-4d32-89c5-38f59c5a89f0",
   "metadata": {},
   "source": [
    "## Step-9: Filtering\n",
    "\n",
    "Filter out documents that do not meet the quality threshold for each annotation. The thresholds are computed based on a distributional\n",
    "analysis as well as manual inspection of samples maintaining the balance between data quality and data volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c54d69-8aee-4f0f-b74c-35dc0609270f",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7991811-b19e-43b5-89ac-b24060c0ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_folder = dq_out\n",
    "# output_folder = filter_out\n",
    "# print (f\"Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c460e05c-aeee-4b53-9dd5-8dfa1afc0ece",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61dea2b0-0e54-4912-8620-886e2b8420ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Disabling for now\n",
    "\n",
    "# %%time \n",
    "\n",
    "# import os\n",
    "\n",
    "# from data_processing.data_access import DataAccessLocal\n",
    "# from filter_transform import (\n",
    "#     filter_columns_to_drop_cli_param,\n",
    "#     filter_criteria_cli_param,\n",
    "#     filter_logical_operator_cli_param,\n",
    "# )\n",
    "# from filter_transform_ray import FilterRayTransformConfiguration\n",
    "\n",
    "\n",
    "# local_conf = {\n",
    "#     \"input_folder\": input_folder,\n",
    "#     \"output_folder\": output_folder,\n",
    "# }\n",
    "\n",
    "# # TODO\n",
    "# # - decide which rules to apply for filtering\n",
    "\n",
    "\n",
    "# # This is just an example criteria to filter\n",
    "# filter_criteria = [\n",
    "#     \"total_num_lines > 10 AND total_num_lines < 90\",\n",
    "#     \"lang_selected = 1\",\n",
    "# ]\n",
    "# filter_logical_operator = \"AND\"\n",
    "# filter_columns_to_drop = [\"lang_selected\", \"hash_column\"]\n",
    "\n",
    "# filter_params = {\n",
    "#     filter_criteria_cli_param: filter_criteria,\n",
    "#     filter_columns_to_drop_cli_param: filter_columns_to_drop,\n",
    "#     filter_logical_operator_cli_param: filter_logical_operator,\n",
    "# }\n",
    "\n",
    "# worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "# launcher_params = {\n",
    "#     # where to run\n",
    "#     \"run_locally\": True,\n",
    "#     # Data access. Only required parameters are specified\n",
    "#     \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "#     # orchestrator\n",
    "#     \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "#     \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "# }\n",
    "\n",
    "\n",
    "# sys.argv = ParamsUtils.dict_to_req(launcher_params | filter_params)\n",
    "# # Create the longer to launch with the blocklist transform.\n",
    "# launcher = RayTransformLauncher(FilterRayTransformConfiguration())\n",
    "# # Launch the ray actor(s) to process the input\n",
    "# launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370950a-2a3a-4143-8218-f9b4808099ba",
   "metadata": {},
   "source": [
    "## Step-10:   Text encoding\n",
    "\n",
    "Encode text for the vector storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20a153fa-fd56-401e-86be-4f7617affcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing input='output/07_dq_out' --> output='output/09_encoder_out'\n"
     ]
    }
   ],
   "source": [
    "# input_folder = filter_out\n",
    "input_folder = dq_out\n",
    "output_folder = encoder_out\n",
    "print (f\"Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "228df6b2-bc62-494b-9697-03ece98d7853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:42:41 INFO - Running locally\n",
      "23:42:41 INFO - text_encoder parameters are : {'content_column_name': 'contents', 'output_embeddings_column_name': 'embeddings', 'model_name': 'sentence-transformers/all-MiniLM-L6-v2'}\n",
      "23:42:41 INFO - data factory data_ is using local data access: input_folder - output/07_dq_out output_folder - output/09_encoder_out\n",
      "23:42:41 INFO - data factory data_ max_files -1, n_sample -1\n",
      "23:42:41 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "23:42:41 INFO - pipeline id pipeline_id\n",
      "23:42:41 INFO - code location None\n",
      "23:42:41 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "23:42:41 INFO - actor creation delay 0\n",
      "23:42:41 INFO - job details {'job category': 'preprocessing', 'job name': 'text_encoder', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-26 23:42:43,579\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=199482)\u001b[0m 23:42:46 INFO - orchestrator started at 2024-08-26 23:42:46\n",
      "\u001b[36m(orchestrate pid=199482)\u001b[0m 23:42:46 INFO - Number of files is 2, source profile {'max_file_size': 0.2225780487060547, 'min_file_size': 0.2152423858642578, 'total_file_size': 0.4378204345703125}\n",
      "\u001b[36m(orchestrate pid=199482)\u001b[0m 23:42:46 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 13.961043549701571, 'object_store': 6.980521773919463}\n",
      "\u001b[36m(orchestrate pid=199482)\u001b[0m 23:42:46 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=199482)\u001b[0m 23:42:46 INFO - Completed 0 files (0.0%)  in 7.128715515136718e-06 min. Waiting for completion\n",
      "\u001b[36m(RayTransformFileProcessor pid=200343)\u001b[0m /home/sujee/apps/anaconda3/envs/data-prep-kit-2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\u001b[36m(RayTransformFileProcessor pid=200343)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(orchestrate pid=199482)\u001b[0m 23:43:39 INFO - Completed processing 2 files in 0.8953257362047832 min\n",
      "\u001b[36m(orchestrate pid=199482)\u001b[0m 23:43:39 INFO - done flushing in 0.0015134811401367188 sec\n",
      "\u001b[36m(RayTransformFileProcessor pid=200344)\u001b[0m /home/sujee/apps/anaconda3/envs/data-prep-kit-2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\u001b[36m(RayTransformFileProcessor pid=200344)\u001b[0m   warnings.warn(\n",
      "23:43:49 INFO - Completed execution in 1.1349955360094706 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 786 ms, sys: 236 ms, total: 1.02 s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from text_encoder_transform_ray import TextEncoderRayTransformConfiguration\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # text_encoder\n",
    "    \"text_encoder_model_name\": MY_CONFIG.EMBEDDING_MODEL,\n",
    "}\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(TextEncoderRayTransformConfiguration())\n",
    "# Launch the ray actor(s) to process the input\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code != 0:\n",
    "    raise Exception (\"❌ Ray job failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e12630-be6b-4188-a925-77117155617b",
   "metadata": {},
   "source": [
    "## Step-11: Copy output to final output dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16dee3b8-31dc-4168-8adb-f2a0a0b5e207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying output from 'output/09_encoder_out' --> 'output/final_output'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output/final_output'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "print (f\"Copying output from '{encoder_out}' --> '{output_folder_final}'\")\n",
    "\n",
    "shutil.rmtree(output_folder_final, ignore_errors=True)\n",
    "shutil.copytree(src=encoder_out, dst=output_folder_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ca71f7-c5e7-4af6-8222-e1eebbe6ef79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
